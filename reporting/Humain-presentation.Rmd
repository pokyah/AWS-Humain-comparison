---
title: "Comparison of T° measurements @ Humain"
author: "Thomas Goossens (CRA-W) - t.goossens@cra.wallonie.be"
date: "`r format(Sys.time(), '%d %B, %Y')`"
always_allow_html: yes
output:
  revealjs::revealjs_presentation:
    center: yes
    highlight: pygments
    incremental: yes
    self_contained: true
    slide_level: 2
    theme: solarized
    transition: slide
    reveal_options:
      previewLinks: false
  md_document:
    toc: no
    toc_depth: 6
    variant: markdown_github
  word_document:
    toc: no
    toc_depth: '6'
  pdf_document: default
  odt_document:
    fig_height: 5
    fig_width: 7
  html_document:
    theme: default
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
#+ ---------------------------------
#' ## Script preparation
#' 
#+ preparation, echo=TRUE, warning=FALSE, message=FALSE, error=FALSE, results='asis'

# Avoid interference with old variables by cleaning the Global Environment
rm(list=ls(all=TRUE))

# Automagically set the wd and the root of all projects 
if (!require("here")) install.packages("here")
library(here)
wd.chr <- here::here()
require(plotly)
require(knitr)
require(broom)

# Defining the .Rmd settings - https://github.com/yihui/knitr/issues/277
knitr::opts_chunk$set(
echo = FALSE,
warning= FALSE,
error= FALSE,
message= FALSE)

# Loading the .RData generared by the .init file
load(file= paste0(wd.chr,"/data-output/ci_ok_model_benchmark.RData"))
#source(paste0(wd.chr,"/init.R"))

#integrate font awesome
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

# todolist

* normalize
* Tester la différence sur les daily max
* Clearness index ==> filtre dataset original pour les valeurs >0.6 ou seuil au-delà duquel,n 30% d'observations (correspondent à périodes d'effets radiatifs)
* apprendre le modèle sur base de cce jue filtré
* Valider le modèles sur les daily max
* Comparaison daily max corr et originaux
* Recréer une série temporelle continue en réintégrant les data nondaily maxima
* comparer les paires de stations proches pour l'ensoleillement pour voir si le modèle est transposable
* Aussi faire une validation sur les data nov a avril 2018

# Context & objectives

## Integration of the RMI + Pameseb AWS networks

* Agromet : providing hourly gridded (1km²) weather datasets for agricultural decision support system  
* Possibility of RMI stations integration for better spatialization ==> interoperability ?   

## Assessing the agreement level between the 2 types of AWS

* previous [work](http://onlinelibrary.wiley.com/doi/10.1002/wea.2158/pdf) by Geoff Jenkins described a comparison method based on regression model correlation study.
* as demonstrated by [Giavarina](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4470095/), correlation studies is not appropriate to study the differences.
* The alternative [Bland-Altman analysis](https://www-users.york.ac.uk/~mb55/meas/ba.pdf) proposed by the authors is used in this study.


# Data presentation

## Period of interest

2 years of hourly records at Humain from `01-nov-2015` to `01-nov-2017` 

## Statistics : 

```{r}
kable(no_extra_filter$statistics.l$tsa_stats.df, caption= "tsa summary stats", digits=2)
```

## Time series
```{r fig.cap = "Time series"}
no_extra_filter$plots$timeseries.plot$tsa.time.plot
```

## Density
```{r fig.cap = "Density"}
no_extra_filter$plots$densities.plot$tsa.dens.plot
```

## Scatter
```{r fig.cap = "Scatter plot"}
no_extra_filter$plots$scatters.plot$tsa.scatter.plot
```

# First insights

## Similarity

Datasets seem in accordance ...

* but to which extent ? 
* How can we quantify this degree of similarity ?  
* Even if a certain degree of similarity : Pameseb seems warmer

## Temperature difference investigation

<i class="fa fa-question-circle"></i>

What can be infered by studying the temperature differences rather than their correlation ?

# Investigating the temperature difference

## Bland-Altman analysis - concept

We can use a [Bland Altman analysis](https://pokyah.github.io/howto/assessing-the-agreement-between-2-quantitative-methods-of-measurements-understanding-the-Bland-Altman-analysis/) to measure the agreement level between the 2 stations

It consists to take observations pair by pair and to plot their `means` vs. their `differences`.

In our case we choose to plot `IRM <i class="fa fa-minus-circle"></i> Pameseb`.

## IRM - Pameseb BA Plot

```{r Plot of mean tsa vs. tsaPameseb - tsaIRM}
no_extra_filter$blandAltman$bland_altman.plot
```

## BA plot interpretation

* For most individuals, measurements made by 2 stations are more likely to be far apart between [`r signif(no_extra_filter$blandAltman$bland_altman.stats.df[1,2], 2)` ; `r signif(no_extra_filter$blandAltman$bland_altman.stats.df[1,4], 2)`] °C.

* 0 is not in the 95% CI of the mean difference [`r signif(no_extra_filter$blandAltman$bland_altman.stats.df[2,6], 2)` ; `r signif(no_extra_filter$blandAltman$bland_altman.stats.df[4,6], 2)` ] <i class="fa fa-hand-point-right"></i> It exists a mean negative __bias__ of `r signif(no_extra_filter$blandAltman$bland_altman.stats.df[1,3],2)`

* The scatter presents a trend going from positive bias to negative bias <i class="fa fa-hand-point-right"></i> proportional bias. The methods do not agree equally through the range of measurements.

* below 10 °C, IRM tends to be warmer (T°diff fluctuates above 0°C) and above 10°C, Pameseb tends to be warmer (T°diff flucutuates below 0°C)

## A potential explanation : station design

The 2 stations are actually different in their design. While the __RMI__ is equipped with a Stevenson radiation screeen mechanically ventiled, __Pameseb__ is equipped with a custom screen without mechanical ventilation

![](./assets/shelters.png){ width=50% }

# Potential effect of the mechanical ventilation

## Supposition

> the lack of ventilation system at Pameseb does not allow to sufficiently compensate the effect of direct solar radiation, leading to an overheating of the Pameseb shelter compared to RMI during situations of high irradiance and low wind.

If `TRUE` and `sufficient` :  

the <i class="fa fa-minus-circle"></i> differences should be inexistant or at least less pronounced at __night__

## IRM - Pameseb BA Plot : Night_only situations
```{r}
night_only$blandAltman$bland_altman.plot
```

## BA plot interpretation : Night_only situations

* For most individuals, measurements made by 2 stations are more likely to be far apart in a narrower range comprised between [`r signif(night_only$blandAltman$bland_altman.stats.df[1,2], 2)` ; `r signif(night_only$blandAltman$bland_altman.stats.df[1,4], 2)`] °C.  

* 0 is not in the 95% CI of the mean difference [`r signif(night_only$blandAltman$bland_altman.stats.df[2,6], 2)` ; `r signif(night_only$blandAltman$bland_altman.stats.df[4,6], 2)` ] <i class="fa fa-hand-point-right"></i> It exists a mean positif __bias__ of `r signif(night_only$blandAltman$bland_altman.stats.df[1,3],2)` (IRM warmer than Pameseb this time)

* The scatter still presents a trend going from positive bias to negative bias <i class="fa fa-hand-point-right"></i>. The ventilation system is not the only explanation to the negative bias. It could actually be the sensitivity of the sensors themselves (and/or non-linear differences in radiation transfer effects from the shelters towards the sensors).

# Solving the disagreement

## Idea

Prior to the integration of the 2 networks in the spatialization process, a correction must be applied to the temperature data of one of the 2 networs. We consider RMI as the gold-standard and will therefore correct the Pameseb AWS network measurements. The value of the correction will be predicted by a correction model able to predict the observed temperature difference :

`corr_pameseb_temp = RMI_temp + predicted_temp_diff`

## Strategy

We can build a correction model that takes into account any combinations of `irradiance`, `windspeed` and `temperature` as predictor(s) to explain the response `temperature differences` (i.e. supervised learning). 
The model definition is performed using the R [mlr package](https://mlr-org.github.io) :

> In mlr a benchmark experiment can be conducted by calling function benchmark on a list of Learners and a list of Tasks. benchmark basically executes resample for each combination of Learner and Task



## cross-validation to assess predictive performance of the models

CV_5_100 : five partitions will be chosen based on the provided coordinates in our task and the partitioning will be repeated 100 times

## Model results


## Correction results
It seems that the model cannot efficiently make the correction for the high temperature.
Maybe should we only compute a model on a subset of the whole dataframe and apply it only on these cases


# Conclusions, perspectives and colofon

## Conclusions

* prior to spatialization, a correction model needs to be applied before integrating the 2 networks

## Perspectives
* Once finished, we plan to submit this work to the ["Weather" Journal of the RMETS](http://rmets.onlinelibrary.wiley.com/hub/journal/10.1002/(ISSN)1477-8696/aims-and-scope/read-full-aims-and-scope.html).
* title idea : 
  > Quantitative intercomparison of 2 years hourly records of temperature and relative
    humidity measured by 2 different types of professional-grade automatic weather stations
    at Humain, Belgium

## Colofon
* This report is still in major revision phase. __Consider it as a first draft version__.  
* This document was generated using R software with the [knitr library](https://deanattali.com/2015/03/24/knitrs-best-hidden-gem-spin/) 
* The interactive plots are rendered from ggplot by [plotly](https://plot.ly).
* The source code of the analysis and this presentation is availbale on [github](https://github.com/pokyah/AWS-Humain-comparison)


<script>
  $(document).ready(function() {
    $items = $('div#TOC li');
    $items.each(function(idx) {
      num_ul = $(this).parentsUntil('#TOC').length;
      $(this).css({'text-indent': num_ul * 10, 'padding-left': 
          0});
    });
    
  });
</script>
  
  
  
  
  
  
  